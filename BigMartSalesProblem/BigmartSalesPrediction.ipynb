{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import mode\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import cross_validation, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#read the train and test data.add a source column to specify its test/train data.combine them for data preprocessing\n",
    "traindata = pd.read_csv(\"/home/arko/Downloads/AnalyticsVidya/BigmartsalesPrediction/Train_UWu5bXk.csv\")\n",
    "testdata = pd.read_csv(\"/home/arko/Downloads/AnalyticsVidya/BigmartsalesPrediction/Test_u94Q5KV.csv\")\n",
    "traindata['source']='train'\n",
    "testdata['source']='test'\n",
    "salesdata = pd.concat([traindata,testdata],ignore_index=True)\n",
    "print traindata.shape,testdata.shape,salesdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#observe the numeric columns\n",
    "salesdata.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#rather than the establishment year, we can have a computed column saying how old the shop is. that makes more sense.\n",
    "#year under consideration is 2013\n",
    "salesdata['outlet_year'] = 2013 - salesdata['Outlet_Establishment_Year']\n",
    "del salesdata['Outlet_Establishment_Year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#the minimum value of item visibility = 0 which does not make sense.\n",
    "#replace the zero values with mean for every product. \n",
    "#We need to identify the products by Product identifier\n",
    "visibility_avg_byProduct = salesdata.pivot_table(values = 'Item_Visibility',index = 'Item_Identifier')\n",
    "miss_bool = (salesdata['Item_Visibility']==0)\n",
    "salesdata.loc[miss_bool,'Item_Visibility'] = salesdata.loc[miss_bool,'Item_Identifier'].apply(lambda x : \n",
    "                                                                                              visibility_avg_byProduct[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Determine another variable with means ratio which will give \n",
    "#some idea about how much importance was given to that product in a store as compared to other stores\n",
    "salesdata['ItemVisibilityMeanRatio'] = salesdata.apply(lambda x : \n",
    "                                       x['Item_Visibility']/visibility_avg_byProduct[x['Item_Identifier']],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#replace the missing values of item Weight with the mean\n",
    "#again for this, we need to identify items by item identifier\n",
    "itemweight_avg_byProduct = salesdata.pivot_table(values = 'Item_Weight',index = 'Item_Identifier')\n",
    "miss_bool = salesdata['Item_Weight'].isnull()\n",
    "salesdata.loc[miss_bool,'Item_Weight'] = salesdata.loc[miss_bool,'Item_Identifier'].apply(\n",
    "    lambda x : itemweight_avg_byProduct[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for outlet size, replace with mode\n",
    "#need to segregate by outlet type\n",
    "outlet_size_avg = salesdata.pivot_table(values='Outlet_Size', columns='Outlet_Type',aggfunc= (lambda x : mode(x).mode[0]) )\n",
    "miss_bool = salesdata['Outlet_Size'].isnull()\n",
    "salesdata.loc[miss_bool,'Outlet_Size'] = salesdata.loc[miss_bool,'Outlet_Type'].apply(lambda x : outlet_size_avg[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get the number of Item types\n",
    "salesdata['Item_Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Item_Type variable has 16 categories which might prove to be very useful in analysis.\n",
    "#So its a good idea to combine them. One way could be to manually assign a new category to each. \n",
    "#But there’s a catch here. If you look at the Item_Identifier, i.e. the unique ID of each item, it starts \n",
    "#with either FD, DR or NC. If you see the categories, these look like being Food, Drinks and Non-Consumables. \n",
    "#So I’ve used the Item_Identifier variable to create a new column\n",
    "salesdata['ItemTypeModified'] = salesdata['Item_Identifier'].apply(lambda x : x[0:2])\n",
    "#rename these\n",
    "salesdata['ItemTypeModified'] = salesdata['ItemTypeModified'].map({'FD':'Food',\n",
    "                                                                'NC':'Non-Consumable',\n",
    "                                                                  'DR':'Drinks'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Correcting typos and difference in representation in categories of Item_Fat_Content\n",
    "salesdata['Item_Fat_Content'] = salesdata['Item_Fat_Content'].replace({'LF': 'Low Fat',\n",
    "                                                                      'low fat': 'Low Fat',\n",
    "                                                                      'reg': 'Regular'})\n",
    "#there were some non-consumables as well and a fat-content should not be specified for them. \n",
    "#So we can also create a separate category for such kind of observations\n",
    "salesdata.loc[salesdata['ItemTypeModified']=='Non-Consumable','Item_Fat_Content'] = 'Non-Edible'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Since scikit-learn accepts only numerical variables, convert all categories of nominal variables into numeric types.\n",
    "#Also, need Outlet_Identifier as a variable as well. So I created a new variable ‘Outlet’ same as Outlet_Identifier \n",
    "#and coded that. Outlet_Identifier should remain as it is, because it will be required in the submission file\n",
    "le = LabelEncoder()\n",
    "salesdata['Outlet'] = le.fit_transform(salesdata['Outlet_Identifier'])\n",
    "variables_to_modify = ['Item_Fat_Content','Outlet_Location_Type','Outlet_Size','ItemTypeModified','Outlet_Type','Outlet']\n",
    "for i in variables_to_modify:\n",
    "    salesdata[i] = le.fit_transform(salesdata[i])\n",
    "#Now do One-Hot encoding for these variables\n",
    "salesdata = pd.get_dummies(salesdata,columns = variables_to_modify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "salesdata.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#now that the preprocessing is done,we re-divide to train and test and delete the unnecessary columns\n",
    "del salesdata['Item_Type']\n",
    "traindata = salesdata.loc[salesdata['source']=='train']\n",
    "testdata = salesdata.loc[salesdata['source']=='test']\n",
    "del traindata['source']\n",
    "testdata.drop(['source','Item_Outlet_Sales'],axis = 1, inplace = True) #for multiple columns. del won't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#writing a generic function for running a model,cross validating and getting predictions\n",
    "target = 'Item_Outlet_Sales'\n",
    "IDcols = ['Item_Identifier','Outlet_Identifier']\n",
    "def get_modelfit(alg,dtrain,dtest,predictors,target = target,IDcols = IDcols):\n",
    "    #run the algorithm on train data\n",
    "    alg.fit(dtrain[predictors],dtrain[target])\n",
    "    #predict on train data\n",
    "    dtrain_preds = alg.predict(dtrain[predictors])\n",
    "    #perform cross validation\n",
    "    cv_score = cross_validation.cross_val_score(alg,dtrain[predictors],dtrain[target],cv=20,scoring = 'mean_squared_error')\n",
    "    cv_score = np.square(np.abs(cv_score))\n",
    "    #get the model report\n",
    "    print \"\\n Model Report \\n\"\n",
    "    print \"RMSE : %.4g \" % np.sqrt(metrics.mean_squared_error(dtrain[target].values,dtrain_preds))\n",
    "    print \"CV Score : Mean - %.4g | Std - %.4g | Min - %.4g | Max - %.4g\" % (np.mean(cv_score),\n",
    "                                                    np.std(cv_score),np.min(cv_score),np.max(cv_score))\n",
    "    #predict on test data\n",
    "    dtest[target] = alg.predict(dtest[predictors])\n",
    "    #generate the prediction file containing the Item and Outlet identifiers and the predictions\n",
    "    IDcols.append(target)\n",
    "    resultdata = pd.DataFrame({x:dtest[x] for x in IDcols})\n",
    "    return resultdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Model Report \n",
      "\n",
      "RMSE : 1128 \n",
      "CV Score : Mean - 1.641e+12 | Std - 2.576e+11 | Min - 1.336e+12 | Max - 2.152e+12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arko/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "#lets try out Linear regression model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "predictors = [x for x in traindata.columns if x not in [target]+ IDcols]\n",
    "alg1 = LinearRegression(normalize = True)\n",
    "result1 = get_modelfit(alg1,traindata,testdata,predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Item_Identifier</th>\n",
       "      <th>Item_Outlet_Sales</th>\n",
       "      <th>Outlet_Identifier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8523</th>\n",
       "      <td>FDW58</td>\n",
       "      <td>1842.0</td>\n",
       "      <td>OUT049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8524</th>\n",
       "      <td>FDW14</td>\n",
       "      <td>1583.5</td>\n",
       "      <td>OUT017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8525</th>\n",
       "      <td>NCN55</td>\n",
       "      <td>1912.5</td>\n",
       "      <td>OUT010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8526</th>\n",
       "      <td>FDQ58</td>\n",
       "      <td>2578.5</td>\n",
       "      <td>OUT017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8527</th>\n",
       "      <td>FDY38</td>\n",
       "      <td>5235.5</td>\n",
       "      <td>OUT027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Item_Identifier  Item_Outlet_Sales Outlet_Identifier\n",
       "8523           FDW58             1842.0            OUT049\n",
       "8524           FDW14             1583.5            OUT017\n",
       "8525           NCN55             1912.5            OUT010\n",
       "8526           FDQ58             2578.5            OUT017\n",
       "8527           FDY38             5235.5            OUT027"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Model Report \n",
      "\n",
      "RMSE : 1058 \n",
      "CV Score : Mean - 1.434e+12 | Std - 2.382e+11 | Min - 1.011e+12 | Max - 1.981e+12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arko/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "#trying Decision tree regressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "predictors = [x for x in traindata.columns if x not in [target]+ IDcols]\n",
    "alg2 = DecisionTreeRegressor(max_depth = 15, min_samples_leaf = 100)\n",
    "result2 = get_modelfit(alg2,traindata,testdata,predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Model Report \n",
      "\n",
      "RMSE : 1068 \n",
      "CV Score : Mean - 1.388e+12 | Std - 2.26e+11 | Min - 1.082e+12 | Max - 1.816e+12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arko/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "#trying out random forest model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "predictors = [x for x in traindata.columns if x not in [target]+IDcols]\n",
    "alg3 = RandomForestRegressor(n_estimators=1000,max_depth=6, min_samples_leaf=100,n_jobs=4)\n",
    "result3 = get_modelfit(alg3, traindata, testdata, predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#we onbserve that decision tree performs the best in terms of RMSE, followed by Random forest and Linear regression.\n",
    "#lets try an ensemble of the tree with decreasing order of importances\n",
    "def get_Ensemblemodelfit(alg1,alg2,dtrain,dtest,predictors,target = target,IDcols = IDcols):\n",
    "    dtrain_preds = pd.DataFrame()\n",
    "    for i in range(1,3):\n",
    "        alg = eval('alg'+str(i))\n",
    "        #run the algorithm on train data\n",
    "        alg.fit(dtrain[predictors],dtrain[target])\n",
    "        #predict on train data\n",
    "        dtrain_preds[i] = alg.predict(dtrain[predictors])\n",
    "        #perform cross validation\n",
    "        cv_score = cross_validation.cross_val_score(alg,dtrain[predictors],dtrain[target],cv=20,\n",
    "                                                    scoring = 'mean_squared_error')\n",
    "        cv_score = np.square(np.abs(cv_score))\n",
    "        #get the model report\n",
    "        print \"\\n Model Report for Algorithm \",i, \"\\n\"\n",
    "        print \"CV Score : Mean - %.4g | Std - %.4g | Min - %.4g | Max - %.4g\" % (np.mean(cv_score),\n",
    "                                                        np.std(cv_score),np.min(cv_score),np.max(cv_score))\n",
    "        #predict on test data\n",
    "        dtest[eval(repr(target+str(i)))] = alg.predict(dtest[predictors])\n",
    "    #get the combined RMSE score\n",
    "    dtrain_preds['final'] = dtrain_preds.mean(axis=1)\n",
    "    print \"Overall RMSE : %.4g \" % np.sqrt(metrics.mean_squared_error(dtrain[target].values,dtrain_preds['final']))\n",
    "    #finally take weighted average as the target column\n",
    "    #I assigned the weights after trail and error to get minimum RMSE \n",
    "    dtest[target] = (dtest['Item_Outlet_Sales1']*0.7) + (dtest['Item_Outlet_Sales2']*0.3)\n",
    "    #generate the prediction file containing the Item and Outlet identifiers and the predictions\n",
    "    IDcols.append(target)\n",
    "    resultdata = pd.DataFrame({x:dtest[x] for x in IDcols})\n",
    "    return resultdata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Model Report for Algorithm  1 \n",
      "\n",
      "CV Score : Mean - 1.641e+12 | Std - 2.576e+11 | Min - 1.336e+12 | Max - 2.152e+12\n",
      "\n",
      " Model Report for Algorithm  2 \n",
      "\n",
      "CV Score : Mean - 1.434e+12 | Std - 2.382e+11 | Min - 1.011e+12 | Max - 1.981e+12\n",
      "\n",
      " Model Report for Algorithm  3 \n",
      "\n",
      "CV Score : Mean - 1.388e+12 | Std - 2.262e+11 | Min - 1.08e+12 | Max - 1.817e+12\n",
      "Overall RMSE : 1066 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arko/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/arko/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#weightage for ensemble : LR .2, DT .5, RF .3\n",
    "result4 = get_Ensemblemodelfit(alg1,alg2,alg3,traindata,testdata,predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result5.to_csv(\"BigmartSales.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Model Report for Algorithm  1 \n",
      "\n",
      "CV Score : Mean - 1.434e+12 | Std - 2.382e+11 | Min - 1.011e+12 | Max - 1.981e+12\n",
      "\n",
      " Model Report for Algorithm  2 \n",
      "\n",
      "CV Score : Mean - 1.387e+12 | Std - 2.261e+11 | Min - 1.076e+12 | Max - 1.815e+12\n",
      "Overall RMSE : 1058 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arko/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/arko/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#weightage for ensemble : DT .7, RF .3\n",
    "result5 = get_Ensemblemodelfit(alg2,alg3,traindata,testdata,predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
