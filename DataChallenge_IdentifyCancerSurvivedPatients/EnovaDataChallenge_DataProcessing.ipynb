{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fancyimpute import KNN\n",
    "import matplotlib\n",
    "import seaborn as se\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime as dt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "matplotlib.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "\n",
    "#load and visualize the training data\n",
    "trainData = pd.read_csv(\"C:/Users/Arko/Downloads/Studies/PersonalResearch/EnovaDataChallenge/participant_files/training_data.csv\")\n",
    "trainData.head()\n",
    "\n",
    "#load and visualize the training data\n",
    "testData = pd.read_csv(\"C:/Users/Arko/Downloads/Studies/PersonalResearch/EnovaDataChallenge/participant_files/testdata.csv\")\n",
    "#testData.head()\n",
    "\n",
    "## explore the relation between some of the categorical variables and the target variable using the trainData\n",
    "\n",
    "#tscore vs survival_7_years\n",
    "plot = se.factorplot('t_score','survival_7_years',data = trainData,kind = 'bar')\n",
    "#Observation: Since there are no definite relation coming out of the categories, it might make sense to club the levels\n",
    "#we do the same during data-processing stage\n",
    "\n",
    "#nscore vs survival_7_years\n",
    "plot = se.factorplot('n_score','survival_7_years',data = trainData,kind = 'bar',palette = 'BuGn')\n",
    "#observation: evidently if the cancer has not spread to lymph nodes, patient has higher chance of survival.\n",
    "#we cannot have any inference about the unknown (NX) state\n",
    "\n",
    "#mscore vs survival_7_years\n",
    "plot = se.factorplot('m_score','survival_7_years',data = trainData,kind = 'bar',palette = 'BuGn')\n",
    "#observation: evidently if the cancer has not spread to distant parts, patient has higher chance of survival.\n",
    "#Since the survival shows less variation among the a,b,c categories of M1( that is cancer has spread to distant parts,\n",
    "#we may club the 3 levels)\n",
    "\n",
    "#stage vs survival_7_years\n",
    "plot = se.factorplot('stage','survival_7_years',data = trainData,kind = 'bar',palette = 'BuGn')\n",
    "#observation: patient with stage IV cancer has lowest survival chance. stages II(A,B) have less variation in count and can be combined\n",
    "\n",
    "#family_history vs survival_7_years\n",
    "plot = se.factorplot('family_history','survival_7_years',data = trainData,kind = 'bar',palette = 'BuGn')\n",
    "#Observation: One peculiar finding is patients with 4 members affected by cancer have a distinctly higher chance of survival\n",
    "#compared to others. However, as indicated by the boxplot, the variation in this level is also high and hence the \n",
    "#uncertainty associated is also more. We may consider this to be a random pattern \n",
    "\n",
    "#previous_cancer vs survival_7_years\n",
    "plot = se.factorplot('previous_cancer','survival_7_years',data = trainData,kind = 'bar',palette = 'BuGn')\n",
    "#observation: Not much difference in survival chance based on whether or not the patient had cancer before\n",
    "\n",
    "#smoker vs survival_7_years\n",
    "plot = se.factorplot('smoker','survival_7_years',data = trainData,kind = 'bar',palette = 'BuGn')\n",
    "#observation: Not much difference in survival chance based on whether or not the patient is a smoker or not\n",
    "#This may suggest that prostate cancer is not affected significantly by smoking but this is only an assumption\n",
    "#since other types of cancer may also be present here\n",
    "\n",
    "#tea vs survival_7_years\n",
    "plot = se.factorplot('tea','survival_7_years',data = trainData,kind = 'bar')\n",
    "#observation: with an exception to 9 cups of tea, the general pattern indicates that patients having \n",
    "#higher number of tea has higher chance of survival. May be having some kind of medicated tea improves survival chance\n",
    "\n",
    "## combine Train and Test Data and perform feature engineering\n",
    "\n",
    "#combine train and test data\n",
    "def combineData():\n",
    "    train = pd.read_csv(\"C:/Users/Arko/Downloads/Studies/PersonalResearch/EnovaDataChallenge/participant_files/training_data.csv\")\n",
    "    test = pd.read_csv(\"C:/Users/Arko/Downloads/Studies/PersonalResearch/EnovaDataChallenge/participant_files/testdata.csv\")\n",
    "    #add an identifier column\n",
    "    train['Identifier'] = \"Train\"\n",
    "    test['Identifier'] = \"Test\"\n",
    "    #remove the Survived column from train so that train and test can be combined\n",
    "    train.drop('survival_7_years',axis=1,inplace = True)\n",
    "    test.drop('survival_7_years',axis=1,inplace = True)\n",
    "    combined = pd.concat([train,test],axis = 0)\n",
    "    combined.reset_index(inplace = True)\n",
    "    #removing unnecessary index column\n",
    "    combined.drop('id',axis = 1,inplace=True)\n",
    "    return(combined)\n",
    "\n",
    "#invoke the function to combine train and test data\n",
    "data = combineData()\n",
    "#get the number of rows and columns\n",
    "print(data.shape)\n",
    "#data.head()\n",
    "#get the summary statistics for Numeric variables\n",
    "data.describe()\n",
    "\n",
    "#we visualize the correlation between the numeric columns\n",
    "#this can be taken into account while imputing missing values for continuous variables \n",
    "corr = data._get_numeric_data().corr()\n",
    "se.heatmap(corr, \n",
    "            xticklabels=corr.columns.values,\n",
    "            yticklabels=corr.columns.values)\n",
    "\n",
    "#variableto be treated : gleason_score\n",
    "#There are only 320 missing values in gleason_score(2% data). hence replacing them with 0\n",
    "data['gleason_score'].fillna(0,inplace = True)\n",
    "#check if the missing values have been replaced\n",
    "#data.describe()\n",
    "\n",
    "#Using the Diagnosis date we are calculating Number of months elapsed after 7 years since diagnosis.\n",
    "#This aligns with the fact that the target column has the survival status at 7 years since diagnosis\n",
    "data['mnths_from_diag']= 84 - pd.to_datetime(data['diagnosis_date']).dt.month.astype(int)\n",
    "#trainData['mnths_from_diag'].head(40)\n",
    "#drop the diagnosis date column since it is no longer required\n",
    "data.drop('diagnosis_date',axis = 1,inplace = True)\n",
    "\n",
    "#for t_score, convert to numeric by converting the values to 1,2,3,4 respectively.\n",
    "#to reduce number of levels, clubbing together the levels a,b,c,d\n",
    "data['t_score'] = data['t_score'].map(lambda x : x[1]).astype(int)\n",
    "#data['t_score'].head()\n",
    "\n",
    "#converting n_score to numeric. Assuming NX to be unknown values, we replace them by -1 \n",
    "data['n_score'] = data['n_score'].map(lambda x: x[1])\n",
    "data['n_score'] = data['n_score'].replace({'X':-1},regex = True)\n",
    "data['n_score'] = data['n_score'].astype(int)\n",
    "#data['n_score'].head(74)\n",
    "\n",
    "#for m_score, convert to numeric by converting the values to 0,1\n",
    "#to reduce number of levels, clubbing together the levels a,b,c,d\n",
    "data['m_score'] = data['m_score'].map(lambda x : x[1]).astype(int)\n",
    "#data['m_score'].head()\n",
    "\n",
    "#mapping stage levels to their numeric equivalents\n",
    "stageDict = {\n",
    "    'I' : '1',\n",
    "    'IIA' : '2',\n",
    "    'IIB' : '2',\n",
    "    'III': '3',\n",
    "    'IV' : '4'\n",
    "}\n",
    "data['stage'] = data['stage'].map(stageDict).astype(int)\n",
    "#data['stage'].head()\n",
    "\n",
    "#We are not sure the most probable value for replacing missing values in race, hence replacing by 0(unknown)\n",
    "#we then create dummy variables\n",
    "data['race'].fillna(0,inplace = True)\n",
    "raceEncoded = pd.get_dummies(data['race'],prefix = \"race_\")\n",
    "data = pd.concat([data,raceEncoded],axis = 1)\n",
    "#data.head()\n",
    "\n",
    "#for following pairs of columns, rather than imputing by mean,median or mode, we use knn imputation(nearest neighbor)\n",
    "#this is based on the intuition that these sets of attributes are related and can be good estimators of each other\n",
    "#we have also considered the correlation between the attributes as depicted above in the corrplot\n",
    "#we first create a function for performing knn imputation\n",
    "#parameters are the data, specified columns and the number of neighbors to be considered\n",
    "def imputeKNN(dat,cols,n):\n",
    "    dat_mat = dat.as_matrix(columns = cols)\n",
    "    dat_imputed = pd.DataFrame(KNN(k=n).complete(dat_mat))\n",
    "    #specify appropriate column names\n",
    "    dat_imputed.columns = cols\n",
    "    #drop the previous columns from the original data\n",
    "    dat.drop(cols,axis = 1,inplace = True)\n",
    "    #add the imputed columns\n",
    "    dat = pd.concat([dat,dat_imputed],axis=1)\n",
    "    return(dat)\n",
    "\n",
    "#in general, age can be imputed based on weight , height and race. The same analogy goes for weight and height. Hence, we \n",
    "#form a subset of these attrbutes and impute the missing values in each by knn imputation( nearest neighbor of 5)\n",
    "pair1 = ['age', 'height','weight','race']\n",
    "data = imputeKNN(data,pair1,3)\n",
    "pair2 = ['family_history', 'first_degree_history','previous_cancer']\n",
    "data = imputeKNN(data,pair2,3)\n",
    "pair3 = ['tumor_diagnosis', 'tumor_6_months','tumor_1_year']\n",
    "data = imputeKNN(data,pair3,5)\n",
    "pair4 = ['psa_diagnosis', 'psa_6_months','psa_1_year']\n",
    "data = imputeKNN(data,pair4,5)\n",
    "#drop the race variable now since it has no more significance and we already have created dummy variables for the same\n",
    "data.drop('race',axis = 1,inplace = True)\n",
    "\n",
    "#tea does not have any prominent correlation with any other column. hence replacing missing values with median\n",
    "data['tea'].fillna(data['tea'].median(),inplace = True)\n",
    "#for smoker column, replace missing values with -1\n",
    "data['smoker'].fillna(-1,inplace = True)\n",
    "\n",
    "#Create dummy variables for side variable\n",
    "sideEncoded = pd.get_dummies(data['side'],prefix = \"side_\")\n",
    "data = pd.concat([data,sideEncoded],axis = 1)\n",
    "#drop the side variable\n",
    "data.drop('side',axis = 1,inplace=True)\n",
    "#data.head()\n",
    "\n",
    "#we calculate increase/decrease of tumor size in first 6months and then next 6 months\n",
    "data['tumor_change_6mnths'] = data['tumor_6_months'] - data['tumor_diagnosis']\n",
    "data['tumor_change_1yr'] = data['tumor_1_year'] - data['tumor_6_months']\n",
    "#we perform the same action for psa level\n",
    "data['psa_change_6mnths'] = data['psa_6_months'] - data['psa_diagnosis']\n",
    "data['psa_change_1yr'] = data['psa_1_year'] - data['psa_6_months']\n",
    "#we will later standardize these columns to bring them into same scale\n",
    "#we drop the parent columns since they will no longer be considered\n",
    "data.drop(['tumor_diagnosis','tumor_6_months','tumor_1_year','psa_diagnosis','psa_6_months','psa_1_year'],axis = 1,inplace = True)\n",
    "\n",
    "#the meaning of the symptoms are not defined. Finding the count of symptoms with assumption that more symptoms means\n",
    "#greater chance the patient has cancer\n",
    "data['symptoms_count'] = data['symptoms'].apply(lambda x : len(str(x).split(',')))\n",
    "\n",
    "#the definition of symptoms are not defined but it is mentioned that the symptoms are predictive\n",
    "#hence we will create dummy variables corresponding to each of the symptoms and include the same in the dataset \n",
    "subset_data = data['symptoms'].str.get_dummies(sep=',')\n",
    "#subset_data.columns\n",
    "data = pd.concat([data,subset_data],axis = 1)\n",
    "#drop the symptoms column since its no longer required\n",
    "data.drop('symptoms',axis = 1,inplace = True)\n",
    "\n",
    "#check the correlation between survival_1_year and survival_7_years on trainData\n",
    "trainData[['survival_1_year','survival_7_years']].corr()\n",
    "#Since these two columns don't have a high correlation let us keep the survival_1_year for now and later analyze\n",
    "#the significance of survival_1_year attribute in predicting the target column\n",
    "#We replace the missing values with -1\n",
    "data['survival_1_year'].fillna(-1,inplace = True)\n",
    "\n",
    "#Scaling all features except index and Identifier columns\n",
    "#scaling can be applicable since we have converted all variables to numeric values\n",
    "features = list(data[data.columns.difference(['Identifier','index'])].columns)\n",
    "#use scikit-learn MinMaxScaler to normalize all the variables\n",
    "scaler = MinMaxScaler()\n",
    "data[features] = scaler.fit_transform(data[features])\n",
    "data.head()\n",
    "\n",
    "#check the name of all the columns at the end of feature engineering \n",
    "data.columns\n",
    "\n",
    "#write the processed data to a file\n",
    "data.to_csv(\"C:/Users/Arko/Downloads/Studies/PersonalResearch/EnovaDataChallenge/participant_files/processedData.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
