{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5\n"
     ]
    }
   ],
   "source": [
    "import urllib.request as urllib2\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import pandas as pd\n",
    "print (urllib2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get current year\n",
    "currentyear = datetime.now().year\n",
    "#print(currentyear)\n",
    "#generate a list of years from 1996 to current year\n",
    "#the data in Securities Class Action Clearinghouse starts from 1996\n",
    "yearslist = list(range(1996,currentyear,1))\n",
    "#yearslist = list(range(1998,2000,1))\n",
    "#print(yearslist)\n",
    "#get the general link\n",
    "baselink = \"http://securities.stanford.edu/filings.html?filter=\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to get number of pages for a year\n",
    "def getPageNumber(baselink):\n",
    "    pagenumberlist = []\n",
    "    flag = True\n",
    "    counter = 1\n",
    "    while flag is True:\n",
    "        link = baselink + \"&page=\" + str(counter)\n",
    "        #print(link)\n",
    "        page = urllib2.urlopen(link).read()\n",
    "        #lxml parameter avoids unnecessary warning\n",
    "        soup = BeautifulSoup(page,\"lxml\")\n",
    "        pagelinks = soup.findAll(\"div\",class_=\"pagination pagination-right\")\n",
    "        for d in pagelinks:\n",
    "            lis = d.find_all('li')\n",
    "        #print(len(lis))\n",
    "        #print(lis[1].get_text())\n",
    "        #print(lis[len(lis)-2].get_text())\n",
    "        pagenumberlist.append(lis[len(lis)-2].get_text())\n",
    "        #following is the logic to check number of pages:\n",
    "        #the pages tab occur maximum in sets of 5 at a time and minimum only 1 page tab will be present\n",
    "        #hence after the counter is incremented by 5, if the following two conditions are encountered\n",
    "        #it means that there are no more pages left and hence control has shifted back to page 1\n",
    "        if(counter != 1):\n",
    "            if(lis[1].get_text() in (\"1\",lis[len(lis)-2].get_text())) and (lis[len(lis)-2].get_text() == \"5\" ):\n",
    "                flag = False\n",
    "        #print(flag) \n",
    "        counter += 5\n",
    "    #print(pagenumberlist)\n",
    "    #print(pagenumberlist[-2])\n",
    "    return(int(pagenumberlist[-2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This is the final code. The below cells are test codes\n",
    "#Now that we have to acquire the page numbers and we can traverse through all the pages and get the filings\n",
    "#information for each page\n",
    "#While doing so, we will also go through each of the years from the list we defined above\n",
    "#we will store the attributes from each row in separate lists \n",
    "filingName = [];filingDate = [];districtCourt = [];exchange =[];ticker = [];\n",
    "for year in yearslist:\n",
    "    #Find number of pages since the html has pagination feature\n",
    "    link = baselink + str(year)\n",
    "    #print(link)\n",
    "    #invoking above function to get number of pages\n",
    "    numberofpages = getPageNumber(link)\n",
    "    #print(numberofpages)\n",
    "    for i in range(numberofpages):\n",
    "        link = baselink + str(year) + \"&page=\" + str(i)\n",
    "        #print(link)\n",
    "        page = urllib2.urlopen(link).read()\n",
    "        #lxml parameter avoids unnecessary warning\n",
    "        soup = BeautifulSoup(page,\"lxml\")\n",
    "        #print(soup.prettify())\n",
    "        #the tbody tag is taken instead of table tag since there is a tr attribute in thead inside table tag as well\n",
    "        filingstable = soup.find('tbody')\n",
    "        for row in filingstable.find_all(\"tr\"):\n",
    "            cells = row.find_all(\"td\")\n",
    "            if(len(cells)>0):\n",
    "                filingName.append(cells[0].get_text().strip())\n",
    "                filingDate.append(cells[1].get_text().strip())\n",
    "                districtCourt.append(cells[2].get_text().strip())\n",
    "                exchange.append(cells[3].get_text().strip())\n",
    "                ticker.append(cells[4].get_text().strip())\n",
    "            else:\n",
    "                print('No data found')\n",
    "                break\n",
    "#store the attributes in a dataframe\n",
    "filingsdata = pd.DataFrame({\n",
    "        \"FilingName\":filingName,\n",
    "        \"FilingDate\":filingDate,\n",
    "        \"DistrictCourt\":districtCourt,\n",
    "        \"Exchange\":exchange,\n",
    "        \"Ticker\":ticker\n",
    "    })\n",
    "#extract the year from the Filing date column\n",
    "filingsdata['FilingYear'] = filingsdata['FilingDate'].str.split('/').str[2]\n",
    "#reorder the columns\n",
    "cols = ['FilingName','FilingDate','FilingYear','DistrictCourt','Exchange','Ticker']\n",
    "filingsdata = filingsdata[cols]\n",
    "#filingsdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FilingName</th>\n",
       "      <th>FilingDate</th>\n",
       "      <th>FilingYear</th>\n",
       "      <th>DistrictCourt</th>\n",
       "      <th>Exchange</th>\n",
       "      <th>Ticker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Access HealthNet, Inc.</td>\n",
       "      <td>12/27/1996</td>\n",
       "      <td>1996</td>\n",
       "      <td>C.D. California</td>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>AHNT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Casino Data Systems</td>\n",
       "      <td>12/24/1996</td>\n",
       "      <td>1996</td>\n",
       "      <td>D. Nevada</td>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>CSDS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Dignity Partners, Inc.</td>\n",
       "      <td>12/19/1996</td>\n",
       "      <td>1996</td>\n",
       "      <td>N.D. California</td>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>DPNR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Open Environment Corporation</td>\n",
       "      <td>12/05/1996</td>\n",
       "      <td>1996</td>\n",
       "      <td>D. Massachusetts</td>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>OPEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Teletek, Inc.</td>\n",
       "      <td>12/02/1996</td>\n",
       "      <td>1996</td>\n",
       "      <td>D. Nevada</td>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>TLTK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      FilingName  FilingDate FilingYear     DistrictCourt  \\\n",
       "20        Access HealthNet, Inc.  12/27/1996       1996   C.D. California   \n",
       "21           Casino Data Systems  12/24/1996       1996         D. Nevada   \n",
       "22        Dignity Partners, Inc.  12/19/1996       1996   N.D. California   \n",
       "23  Open Environment Corporation  12/05/1996       1996  D. Massachusetts   \n",
       "24                 Teletek, Inc.  12/02/1996       1996         D. Nevada   \n",
       "\n",
       "   Exchange Ticker  \n",
       "20   NASDAQ   AHNT  \n",
       "21   NASDAQ   CSDS  \n",
       "22   NASDAQ   DPNR  \n",
       "23   NASDAQ   OPEN  \n",
       "24   NASDAQ   TLTK  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove empty cells if any\n",
    "filter = filingsdata[\"FilingName\"] != \"\"\n",
    "filingsdata = filingsdata[filter]\n",
    "filingsdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#write the final dataframe to a csv file\n",
    "filingsdata.to_csv(\"C:/Users/Arko/Downloads/Studies/PersonalResearch/NetworkProject/FilingsData_SCAdata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test code(this technique works for pagination and getting correct number of pages)\n",
    "#Find number of pages since the html has pagination feature\n",
    "pagenumberlist = []\n",
    "flag = True\n",
    "counter = 1\n",
    "while flag is True:\n",
    "    link = baselink + str(yearslist[1]) + \"&page=\" + str(counter)\n",
    "    print(link)\n",
    "    page = urllib2.urlopen(link).read()\n",
    "    #lxml parameter avoids unnecessary warning\n",
    "    soup = BeautifulSoup(page,\"lxml\")\n",
    "    pagelinks = soup.findAll(\"div\",class_=\"pagination pagination-right\")\n",
    "    for d in pagelinks:\n",
    "        lis = d.find_all('li')\n",
    "    #print(len(lis))\n",
    "    print(lis[1].get_text())\n",
    "    print(lis[len(lis)-2].get_text())\n",
    "    pagenumber.append(lis[len(lis)-2].get_text())\n",
    "    if(counter != 1):\n",
    "        if(lis[1].get_text() == \"1\") and (lis[len(lis)-2].get_text() == \"5\" ):\n",
    "            flag = False\n",
    "    print(flag) \n",
    "    counter += 5\n",
    "print(pagenumberlist)\n",
    "print(pagenumberlist[-2])\n",
    "pagenumber = pagenumberlist[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test code(this algorithm for pagination will not work)\n",
    "#Find number of pages since the html has pagination feature\n",
    "link = baselink + str(yearslist[1])\n",
    "print(link)\n",
    "page = urllib2.urlopen(link).read()\n",
    "#lxml parameter avoids unnecessary warning\n",
    "soup = BeautifulSoup(page,\"lxml\")\n",
    "pages = soup.findAll(\"div\",class_=\"pagination pagination-right\")\n",
    "for d in pages:\n",
    "    lis = d.find_all('li')\n",
    "print(len(lis))\n",
    "#Number of pages will be total number of li tags minus the li tags next and previous tags\n",
    "if(len(lis) < 2):\n",
    "    numberofpages = len(lis)\n",
    "else:\n",
    "    numberofpages = len(lis) - 2\n",
    "print(numberofpages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test code\n",
    "#the tbody tag is taken instead of table tag since there is a tr attribute in thead inside table tag as well\n",
    "filingstable = soup.find('tbody')\n",
    "#filingstable\n",
    "#we will store the attributes from each row in separate lists \n",
    "filingName = [];filingDate = [];districtCourt = [];exchange =[];ticker = []\n",
    "for row in filingstable.find_all(\"tr\"):\n",
    "    cells = row.find_all(\"td\")\n",
    "    if(len(cells)>0):\n",
    "        filingName.append(cells[0].get_text().strip())\n",
    "        filingDate.append(cells[1].get_text().strip())\n",
    "        districtCourt.append(cells[2].get_text().strip())\n",
    "        exchange.append(cells[3].get_text().strip())\n",
    "        ticker.append(cells[4].get_text().strip())\n",
    "    else:\n",
    "        print('No data found')\n",
    "        break\n",
    "#print(ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test code\n",
    "#store the attributes in a dataframe\n",
    "filingsdata = pd.DataFrame()\n",
    "filingsdata = pd.DataFrame({\n",
    "        \"FilingName\":filingName,\n",
    "        \"FilingDate\":filingDate,\n",
    "        \"DistrictCourt\":districtCourt,\n",
    "        \"Exchange\":exchange,\n",
    "        \"Ticker\":ticker\n",
    "    })\n",
    "#reorder the columns\n",
    "cols = ['FilingName','FilingDate','DistrictCourt','Exchange','Ticker']\n",
    "filingsdata = filingsdata[cols]\n",
    "filingsdata"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
