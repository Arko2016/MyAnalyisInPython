{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following notebook describes the basic steps for using Pandas in Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "refer link : \n",
    "\n",
    "http://www.dataschool.io/best-python-pandas-resources/\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2015/04/comprehensive-guide-data-exploration-sas-using-python-numpy-scipy-matplotlib-pandas/\n",
    "\n",
    "https://chrisalbon.com/python/pandas_join_merge_dataframe.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read a dataset of Chipotle orders directly from a URL and store the results in a DataFrame\n",
    "data = pd.read_table('http://bit.ly/chiporders')\n",
    "#every dataframe will have an index\n",
    "print(data.index)\n",
    "#examine first 6 rows\n",
    "print(data.head(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read a dataset of movie reviewers (modifying the default parameter values for read_table)\n",
    "user_cols = ['user_id', 'age', 'gender', 'occupation', 'zip_code']\n",
    "#first view the original data\n",
    "users.orig = pd.read_table('http://bit.ly/movieusers')\n",
    "print(users.orig.head(5))\n",
    "#Now modify appropriately\n",
    "users = pd.read_table('http://bit.ly/movieusers', sep='|', header=None, names=user_cols)\n",
    "print(users.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Series \n",
    "#It is a 1 dimensional indexed array\n",
    "# read_csv is equivalent to read_table, except it assumes a comma separator\n",
    "ufo = pd.read_csv('http://bit.ly/uforeports')\n",
    "print(ufo.head(5))\n",
    "#each column represents a series\n",
    "print(ufo['City'].head(5))\n",
    "#this is same as the following\n",
    "print(ufo.City.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create a new column(or Series) Location using City and State\n",
    "ufo['Location'] = ufo['City'] + ',' + ufo['State']\n",
    "print(ufo.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#in pandas, methods end with parenthesis, but attributes don't\n",
    "print(ufo.head())\n",
    "#get summary statistics -> method\n",
    "print(ufo.describe(include = 'all'))\n",
    "#get dimensions -> attribute\n",
    "print(ufo.shape)\n",
    "#get the datatypes -> attribute\n",
    "print(ufo.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get column names\n",
    "print(ufo.columns)\n",
    "# rename two of the columns by using the 'rename' method\n",
    "ufo.rename(columns={'Colors Reported':'Colors_Reported', 'Shape Reported':'Shape_Reported'}, inplace=True)\n",
    "print(ufo.columns)\n",
    "# replace all of the column names by overwriting the 'columns' attribute\n",
    "ufo_cols = ['city', 'colors reported', 'shape reported', 'state', 'time','location']\n",
    "ufo.columns = ufo_cols\n",
    "print(ufo.columns)\n",
    "# replace the column names during the file reading process by using the 'names' parameter\n",
    "ufo_cols1 = ['city', 'colors reported', 'shape reported', 'state', 'time']\n",
    "ufo1 = pd.read_csv('http://bit.ly/uforeports', header=0, names=ufo_cols1)\n",
    "print(ufo1.columns)\n",
    "# replace all spaces with underscores in the column names by using the 'str.replace' method\n",
    "ufo1.columns = ufo1.columns.str.replace(' ', '_')\n",
    "print(ufo1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#read a dataframe and remove columns\n",
    "ufo = pd.read_csv('http://bit.ly/uforeports')\n",
    "# remove multiple columns at once\n",
    "#axis = 1 refers to columns\n",
    "ufo.drop(['City', 'State'], axis=1, inplace=True)\n",
    "print(ufo.head())\n",
    "#remove multiple columns\n",
    "#axis = 0 refers to rows\n",
    "ufo.drop([0,1],axis = 0,inplace = True)\n",
    "print(ufo.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Sorting\n",
    "#read a dataset\n",
    "movies = pd.read_csv('http://bit.ly/imdbratings')\n",
    "print(movies.head())\n",
    "#sort the title series in descending order\n",
    "print(movies['title'].sort_values(ascending = False).head())\n",
    "#sort the entire dataframe in descending order using title\n",
    "print(movies.sort_values('title',ascending = False).head())\n",
    "# sort the DataFrame first by 'content_rating', then by 'duration'\n",
    "#Note: Content in Descending order and Duration in ascending order\n",
    "movies.sort_values(['content_rating','duration'],ascending= [False,True]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Filtering\n",
    "#Filter the DataFrame rows to only show movies with a 'duration' of at least 200 minutes\n",
    "#sort in descending by duration\n",
    "print(movies[movies['duration']>=200].sort_values('duration',ascending = False).head())\n",
    "#select a series(or column) after filtering\n",
    "print(movies[movies['duration']>=200]['genre'].head())\n",
    "#apply multiple filters\n",
    "#Note: should give each of the conditions in brackets\n",
    "print(movies[(movies['duration']>=200) & (movies['genre'] == 'Drama')])\n",
    "#select the movies whose genre is Crime, Drama or Action\n",
    "print(movies[movies['genre'].isin(['Crime','Drama','Action'])].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#When reading from a file, how do I read in only a subset of the columns?\n",
    "# read a dataset of UFO reports into a DataFrame, and check the columns\n",
    "ufo = pd.read_csv('http://bit.ly/uforeports')\n",
    "print(ufo.columns)\n",
    "# specify which columns to include by name\n",
    "#specify Number of Rows to read using nrows\n",
    "ufo = pd.read_csv('http://bit.ly/uforeports', usecols=['City', 'State'], nrows = 5)\n",
    "# or equivalently, specify columns by position\n",
    "ufo = pd.read_csv('http://bit.ly/uforeports', usecols=[0, 4], nrows = 4)\n",
    "print(ufo.columns)\n",
    "print(ufo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ufo = pd.read_csv('http://bit.ly/uforeports',nrows = 20)\n",
    "#itearate through a column\n",
    "for c in ufo['City']:\n",
    "    print(c)\n",
    "#iterate through a dataframe\n",
    "for index,row in ufo.iterrows():\n",
    "    print(index,row['City'],row['State'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#include only numeric columns\n",
    "drinks = pd.read_csv('http://bit.ly/drinksbycountry')\n",
    "print(drinks.select_dtypes(include = [np.number]).dtypes)\n",
    "#find the mean of each numeric column\n",
    "#since we have to find mean for each column, the operation has to be row-wise\n",
    "#this is symbolized by axis = 0 or axis = 'index'\n",
    "print(drinks.mean(axis = 0))\n",
    "print(drinks.mean(axis = 'index'))\n",
    "#find the mean of each row\n",
    "#since we have to find mean for each row, the operation has to be column-wise\n",
    "#this is symbolized by axis = 1 or axis = 'columns'\n",
    "print(drinks.mean(axis = 1).head())\n",
    "print(drinks.mean(axis = 'columns').head())\n",
    "#get the value of beer servings at a given index, eg. 23\n",
    "print(drinks.loc[23,'beer_servings'])\n",
    "#set a column as the index\n",
    "drinks.set_index('country',inplace=True)\n",
    "print(drinks.index)\n",
    "#NOTE: now country will be the index and will no longer be a column\n",
    "print(drinks.head())\n",
    "print(drinks.columns)\n",
    "#Now a value of country can be used for selection\n",
    "print(drinks.loc['Brazil','beer_servings'])\n",
    "#any Series can also be sorted by its index\n",
    "print(drinks['continent'].value_counts().sort_index())\n",
    "#reset index \n",
    "drinks.reset_index(inplace = True)\n",
    "print(drinks.head())\n",
    "#get the 75% quantile value(Q3) for spirit servings\n",
    "#first get the summary stats using describe(). Then extract the value for 75% quantile using indexing\n",
    "print(drinks.describe().loc['75%','spirit_servings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#convert all the elements of a column to upper\n",
    "orders = pd.read_table('http://bit.ly/chiporders')\n",
    "print(orders['item_name'].str.upper().head())\n",
    "#filter those orders where there is Chicken(as a substring) in the item_name\n",
    "print(orders[orders['item_name'].str.contains('Chicken')].head())\n",
    "#replace the '[' and ']' with empty spaces\n",
    "print(orders['choice_description'].str.replace('[','').str.replace(']','').head())\n",
    "#same thing can also be achieved through regex\n",
    "print(orders['choice_description'].str.replace('[\\[\\]]','').head())\n",
    "#convert the item price to float by first removing $ sign and converting to float\n",
    "#then calculate mean\n",
    "print(orders['item_price'].str.replace('$','').astype(float).mean())\n",
    "#check if an item_name contains Chicken. Then replace it with 1 else 0\n",
    "print(orders['item_name'].str.contains('Chicken').astype(int).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "drinks = pd.read_csv('http://bit.ly/drinksbycountry')\n",
    "print(drinks.dtypes)\n",
    "#change datatypes\n",
    "drinks['beer_servings'] = drinks['beer_servings'].astype('float64')\n",
    "print(drinks.dtypes)\n",
    "# alternatively, change the data type of a Series while reading in a file\n",
    "drinks = pd.read_csv('http://bit.ly/drinksbycountry', dtype={'beer_servings':float})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#GroupBy usage\n",
    "drinks = pd.read_csv('http://bit.ly/drinksbycountry')\n",
    "print(drinks.head())\n",
    "#find the mean servings for each continent\n",
    "#we need to first group the data by continent and then find mean servings for eacg group\n",
    "print(drinks.groupby('continent')['beer_servings'].mean())\n",
    "# multiple aggregation functions can be applied simultaneously\n",
    "print(drinks.groupby('continent')['beer_servings'].agg(['count', 'mean', 'min', 'max']))\n",
    "#for each continent, find mean for all other columns\n",
    "print(drinks.groupby('continent').mean())\n",
    "#allow plots to appear in notebook\n",
    "%matplotlib inline\n",
    "drinks.groupby('continent').mean().plot(kind = 'bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "movies = pd.read_csv('http://bit.ly/imdbratings')\n",
    "print(movies.head())\n",
    "#exploring a Categorical Column(Series) -> genre\n",
    "print(movies['genre'].describe())\n",
    "# count how many times each value in the Series occurs\n",
    "print(movies['genre'].value_counts().sort_values())\n",
    "#NOTE: value_counts does not include missing values by default\n",
    "# display percentages instead of raw counts. Sort by the percentages\n",
    "print(movies['genre'].value_counts(normalize = True).sort_values(ascending = False))\n",
    "# display the unique values in the Series genre\n",
    "print(movies['genre'].unique())\n",
    "#  count the number of unique values in the Series\n",
    "print(movies['genre'].nunique())\n",
    "#get a bar chart for the counts\n",
    "%matplotlib inline\n",
    "movies['genre'].value_counts(normalize = True).plot(kind = 'bar')\n",
    "# compute a cross-tabulation of two Series\n",
    "pd.crosstab(movies['genre'],movies['content_rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "movies = pd.read_csv('http://bit.ly/imdbratings')\n",
    "print(movies.head())\n",
    "#exploring a Numeric Column(Series) -> duration\n",
    "print(movies['duration'].describe())\n",
    "# histogram of the 'duration' Series (shows the distribution of a numerical variable)\n",
    "%matplotlib inline\n",
    "movies['duration'].plot(kind = 'hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#handling Missing values\n",
    "#NaN -> Not a Number\n",
    "#read_csv detects missing values (by default) when reading the file, and replaces them with this special value\n",
    "ufo = pd.read_csv('http://bit.ly/uforeports')\n",
    "# 'isnull' returns a DataFrame of booleans (True if missing, False if not missing)\n",
    "print(ufo.isnull().tail())\n",
    "# 'notnull' returns the opposite of 'isnull' (True if not missing, False if missing)\n",
    "print(ufo.notnull().tail())\n",
    "# count the number of missing values in each Series/Column\n",
    "print(ufo.isnull().sum())\n",
    "# use the 'isnull' Series method to filter the DataFrame rows\n",
    "print(ufo[ufo.isnull()].tail())\n",
    "#similarly get the rows without any missing values\n",
    "print(ufo[ufo.notnull()].tail())\n",
    "#get the observations where Colors Reported is not null\n",
    "print(ufo[ufo['Colors Reported'].notnull()].tail())\n",
    "#get shape\n",
    "print(ufo.shape)\n",
    "# if 'any' values are missing IN A ROW, then drop that row. Then get the dimensions\n",
    "ufo1 = ufo.dropna(how = 'any')\n",
    "print(ufo1.shape)\n",
    "#if all values in a row are missing, delete that row\n",
    "print(ufo.dropna(how = 'all').shape)\n",
    "# if 'any' values are missing in a row (considering only 'City' and 'Shape Reported'), then drop that row\n",
    "print(ufo.dropna(subset = ['City','Shape Reported'],how = 'any').shape)\n",
    "# 'value_counts' does not include missing values by default\n",
    "print(ufo['Shape Reported'].value_counts().head())\n",
    "# explicitly include missing values while calculating count\n",
    "print(ufo['Shape Reported'].value_counts(dropna=False).head())\n",
    "# fill in missing values with a specified value\n",
    "ufo['Shape Reported'].fillna(value='MISC', inplace=True)\n",
    "#backward filling of missing values\n",
    "ufo['Shape Reported'].fillna(method = 'bfill').tail()\n",
    "#forward filling of missing values\n",
    "ufo['Shape Reported'].fillna(method = 'ffill').tail()\n",
    "#confirm that they have been replaced. The value_counts will change\n",
    "print(ufo['Shape Reported'].value_counts().head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Concat and Join\n",
    "#create dataframes\n",
    "raw_data = {\n",
    "        'subject_id': ['1', '2', '3', '4', '5'],\n",
    "        'first_name': ['Alex', 'Amy', 'Allen', 'Alice', 'Ayoung'],\n",
    "        'last_name': ['Anderson', 'Ackerman', 'Ali', 'Aoni', 'Atiches']}\n",
    "df_a = pd.DataFrame(raw_data, columns = ['subject_id', 'first_name', 'last_name'])\n",
    "raw_data = {\n",
    "        'subject_id': ['4', '5', '6', '7', '8'],\n",
    "        'first_name': ['Billy', 'Brian', 'Bran', 'Bryce', 'Betty'],\n",
    "        'last_name': ['Bonder', 'Black', 'Balwner', 'Brice', 'Btisan']}\n",
    "df_b = pd.DataFrame(raw_data, columns = ['subject_id', 'first_name', 'last_name'])\n",
    "df_b\n",
    "raw_data = {\n",
    "        'subject_id': ['1', '2', '3', '4', '5', '7', '8', '9', '10', '11'],\n",
    "        'test_id': [51, 15, 15, 61, 16, 14, 15, 1, 61, 16]}\n",
    "df_n = pd.DataFrame(raw_data, columns = ['subject_id','test_id'])\n",
    "df_n\n",
    "#concat is like rbind(axis = 0) and cbind(axis =1) in R. No specific ids are required\n",
    "df_new = pd.concat([df_a, df_b],axis = 0)\n",
    "print(df_new)\n",
    "print(pd.concat([df_a, df_b], axis=1))\n",
    "#merge is like Join in SQL. So ids are required\n",
    "#Merge two dataframes along the subject_id value\n",
    "print(pd.merge(df_new, df_n, on='subject_id'))\n",
    "#Merge two dataframes with both the left and right dataframes using the subject_id key\n",
    "#this can be useful if the primary keys' names are different\n",
    "print(pd.merge(df_new, df_n, left_on='subject_id', right_on='subject_id'))\n",
    "#outer join\n",
    "print(pd.merge(df_a, df_b, on='subject_id', how='outer'))\n",
    "#inner join\n",
    "print(pd.merge(df_a, df_b, on='subject_id', how='inner'))\n",
    "#right join\n",
    "print(pd.merge(df_a, df_b, on='subject_id', how='right'))\n",
    "#left join\n",
    "print(pd.merge(df_a, df_b, on='subject_id', how='left'))\n",
    "#Merge while adding a suffix to duplicate column names\n",
    "print(pd.merge(df_a, df_b, on='subject_id', how='left', suffixes=('_left', '_right')))\n",
    "#Merge based on indexes\n",
    "print(pd.merge(df_a, df_b, right_index=True, left_index=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
